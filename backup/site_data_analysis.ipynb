{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Site Data Analysis and Metadata Generation\n",
        "\n",
        "This notebook analyzes all site data files and generates comprehensive metadata including:\n",
        "- Date ranges and year coverage\n",
        "- Missing values analysis\n",
        "- Data quality checks\n",
        "- Statistical summaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_csv_file(file_path, file_name):\n",
        "    \"\"\"\n",
        "    Analyze a single CSV file and extract metadata.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    file_path : str\n",
        "        Full path to the CSV file\n",
        "    file_name : str\n",
        "        Name of the CSV file\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    dict : Dictionary containing file metadata\n",
        "    \"\"\"\n",
        "    print(f\"\\n  Analyzing: {file_name}\")\n",
        "    print(f\"  {'-'*76}\")\n",
        "    \n",
        "    try:\n",
        "        # Read the CSV file\n",
        "        df = pd.read_csv(file_path)\n",
        "        \n",
        "        # Basic file information\n",
        "        metadata = {\n",
        "            'file_name': file_name,\n",
        "            'file_path': file_path,\n",
        "            'total_rows': len(df),\n",
        "            'total_columns': len(df.columns),\n",
        "            'columns': list(df.columns),\n",
        "            'dtypes': df.dtypes.to_dict(),\n",
        "            'file_size_mb': os.path.getsize(file_path) / (1024 * 1024)\n",
        "        }\n",
        "        \n",
        "        # Date and time analysis\n",
        "        if 'day' in df.columns:\n",
        "            df['day'] = pd.to_datetime(df['day'])\n",
        "            metadata['date_range'] = {\n",
        "                'start_date': df['day'].min().strftime('%Y-%m-%d'),\n",
        "                'end_date': df['day'].max().strftime('%Y-%m-%d'),\n",
        "                'start_datetime': df['day'].min(),\n",
        "                'end_datetime': df['day'].max(),\n",
        "                'total_days': (df['day'].max() - df['day'].min()).days + 1,\n",
        "                'years': sorted(df['day'].dt.year.unique().tolist()),\n",
        "                'year_range': f\"{df['day'].min().year} - {df['day'].max().year}\",\n",
        "                'unique_dates': df['day'].nunique()\n",
        "            }\n",
        "        \n",
        "        # Missing values analysis\n",
        "        missing_values = df.isnull().sum()\n",
        "        missing_percent = (missing_values / len(df)) * 100\n",
        "        \n",
        "        metadata['missing_values'] = {\n",
        "            'total_missing': missing_values.sum(),\n",
        "            'columns_with_missing': missing_values[missing_values > 0].to_dict(),\n",
        "            'missing_percentage': missing_percent[missing_percent > 0].to_dict(),\n",
        "            'complete_rows': len(df) - df.isnull().any(axis=1).sum(),\n",
        "            'rows_with_missing': df.isnull().any(axis=1).sum()\n",
        "        }\n",
        "        \n",
        "        # Statistical summary for numeric columns\n",
        "        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "        if numeric_cols:\n",
        "            metadata['statistics'] = df[numeric_cols].describe().to_dict()\n",
        "            metadata['numeric_columns'] = numeric_cols\n",
        "        \n",
        "        # Data quality checks\n",
        "        metadata['quality_checks'] = {\n",
        "            'duplicate_rows': df.duplicated().sum(),\n",
        "            'negative_values': {},\n",
        "            'zero_values': {},\n",
        "            'outliers_detected': {}\n",
        "        }\n",
        "        \n",
        "        # Check for negative values in numeric columns\n",
        "        for col in numeric_cols:\n",
        "            negative_count = (df[col] < 0).sum()\n",
        "            zero_count = (df[col] == 0).sum()\n",
        "            if negative_count > 0:\n",
        "                metadata['quality_checks']['negative_values'][col] = negative_count\n",
        "            if zero_count > 0:\n",
        "                metadata['quality_checks']['zero_values'][col] = zero_count\n",
        "            \n",
        "            # Simple outlier detection (values beyond 3 standard deviations)\n",
        "            if df[col].std() > 0:\n",
        "                mean_val = df[col].mean()\n",
        "                std_val = df[col].std()\n",
        "                outliers = ((df[col] < mean_val - 3*std_val) | (df[col] > mean_val + 3*std_val)).sum()\n",
        "                if outliers > 0:\n",
        "                    metadata['quality_checks']['outliers_detected'][col] = outliers\n",
        "        \n",
        "        # Time-related analysis\n",
        "        if all(col in df.columns for col in ['hour', 'minute']):\n",
        "            metadata['time_analysis'] = {\n",
        "                'hour_range': f\"{df['hour'].min()} - {df['hour'].max()}\",\n",
        "                'minute_range': f\"{df['minute'].min()} - {df['minute'].max()}\",\n",
        "                'unique_hours': sorted(df['hour'].unique().tolist()),\n",
        "                'unique_minutes': sorted(df['minute'].unique().tolist()),\n",
        "                'expected_records_per_day': df.groupby('day').size().describe().to_dict() if 'day' in df.columns else None\n",
        "            }\n",
        "        \n",
        "        # Print summary\n",
        "        print(f\"    ✓ Rows: {metadata['total_rows']:,}\")\n",
        "        print(f\"    ✓ Columns: {metadata['total_columns']}\")\n",
        "        if 'date_range' in metadata:\n",
        "            print(f\"    ✓ Date Range: {metadata['date_range']['start_date']} to {metadata['date_range']['end_date']}\")\n",
        "            print(f\"    ✓ Years: {metadata['date_range']['year_range']}\")\n",
        "        print(f\"    ✓ Missing Values: {metadata['missing_values']['total_missing']:,} ({metadata['missing_values']['total_missing']/len(df)*100:.2f}%)\")\n",
        "        print(f\"    ✓ Duplicate Rows: {metadata['quality_checks']['duplicate_rows']:,}\")\n",
        "        \n",
        "        return metadata\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"    ✗ ERROR analyzing {file_name}: {str(e)}\")\n",
        "        return {\n",
        "            'file_name': file_name,\n",
        "            'error': str(e)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def aggregate_site_summary(files_metadata):\n",
        "    \"\"\"\n",
        "    Aggregate metadata across all files in a site.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    files_metadata : dict\n",
        "        Dictionary of file metadata\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    dict : Aggregated summary statistics\n",
        "    \"\"\"\n",
        "    summary = {\n",
        "        'total_files': len(files_metadata),\n",
        "        'total_rows': 0,\n",
        "        'total_missing_values': 0,\n",
        "        'all_years': set(),\n",
        "        'date_range_all': {'start': None, 'end': None},\n",
        "        'all_columns': set()\n",
        "    }\n",
        "    \n",
        "    for file_name, file_meta in files_metadata.items():\n",
        "        if 'error' in file_meta:\n",
        "            continue\n",
        "            \n",
        "        summary['total_rows'] += file_meta.get('total_rows', 0)\n",
        "        summary['total_missing_values'] += file_meta.get('missing_values', {}).get('total_missing', 0)\n",
        "        \n",
        "        if 'date_range' in file_meta:\n",
        "            years = file_meta['date_range'].get('years', [])\n",
        "            summary['all_years'].update(years)\n",
        "            \n",
        "            start = file_meta['date_range'].get('start_datetime')\n",
        "            end = file_meta['date_range'].get('end_datetime')\n",
        "            \n",
        "            if start and (summary['date_range_all']['start'] is None or start < summary['date_range_all']['start']):\n",
        "                summary['date_range_all']['start'] = start\n",
        "            if end and (summary['date_range_all']['end'] is None or end > summary['date_range_all']['end']):\n",
        "                summary['date_range_all']['end'] = end\n",
        "        \n",
        "        if 'columns' in file_meta:\n",
        "            summary['all_columns'].update(file_meta['columns'])\n",
        "    \n",
        "    summary['all_years'] = sorted(list(summary['all_years']))\n",
        "    summary['year_range'] = f\"{min(summary['all_years'])} - {max(summary['all_years'])}\" if summary['all_years'] else \"N/A\"\n",
        "    \n",
        "    if summary['date_range_all']['start']:\n",
        "        summary['date_range_all']['start_str'] = summary['date_range_all']['start'].strftime('%Y-%m-%d')\n",
        "        summary['date_range_all']['end_str'] = summary['date_range_all']['end'].strftime('%Y-%m-%d')\n",
        "        summary['date_range_all']['total_days'] = (summary['date_range_all']['end'] - summary['date_range_all']['start']).days + 1\n",
        "    \n",
        "    summary['all_columns'] = sorted(list(summary['all_columns']))\n",
        "    \n",
        "    return summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_site_data(site_path):\n",
        "    \"\"\"\n",
        "    Analyze all CSV files in a site directory and generate comprehensive metadata.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    site_path : str\n",
        "        Path to the site directory (e.g., 'Site Data/Site 01')\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    dict : Dictionary containing metadata for all files in the site\n",
        "    \"\"\"\n",
        "    site_name = os.path.basename(site_path)\n",
        "    site_metadata = {\n",
        "        'site_name': site_name,\n",
        "        'files': {}\n",
        "    }\n",
        "    \n",
        "    # Get all CSV files in the site directory\n",
        "    csv_files = sorted([f for f in os.listdir(site_path) if f.endswith('.csv')])\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"ANALYZING: {site_name}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    for csv_file in csv_files:\n",
        "        file_path = os.path.join(site_path, csv_file)\n",
        "        file_metadata = analyze_csv_file(file_path, csv_file)\n",
        "        site_metadata['files'][csv_file] = file_metadata\n",
        "    \n",
        "    # Aggregate site-level statistics\n",
        "    site_metadata['summary'] = aggregate_site_summary(site_metadata['files'])\n",
        "    \n",
        "    return site_metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_site_summary(site_metadata):\n",
        "    \"\"\"\n",
        "    Print a formatted summary of site metadata.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    site_metadata : dict\n",
        "        Site metadata dictionary\n",
        "    \"\"\"\n",
        "    summary = site_metadata['summary']\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"SITE SUMMARY: {site_metadata['site_name']}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Total Files: {summary['total_files']}\")\n",
        "    print(f\"Total Rows: {summary['total_rows']:,}\")\n",
        "    print(f\"Total Missing Values: {summary['total_missing_values']:,}\")\n",
        "    \n",
        "    if summary['date_range_all']['start']:\n",
        "        print(f\"\\nDate Range (All Files):\")\n",
        "        print(f\"  Start: {summary['date_range_all']['start_str']}\")\n",
        "        print(f\"  End:   {summary['date_range_all']['end_str']}\")\n",
        "        print(f\"  Total Days: {summary['date_range_all']['total_days']:,}\")\n",
        "    \n",
        "    if summary['all_years']:\n",
        "        print(f\"\\nYear Coverage: {summary['year_range']}\")\n",
        "        print(f\"  Years: {', '.join(map(str, summary['all_years']))}\")\n",
        "    \n",
        "    print(f\"\\nAll Columns Found: {', '.join(summary['all_columns'])}\")\n",
        "    print(f\"{'='*80}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analyze All Sites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 3 site(s) to analyze:\n",
            "  - Site 01\n",
            "  - Site 02\n",
            "  - Site 03\n"
          ]
        }
      ],
      "source": [
        "# Set the base path to Site Data folder\n",
        "base_path = \"/Users/IRFAN/Desktop/Irradiance-forecasting/Site Data\"\n",
        "\n",
        "# Get all site directories\n",
        "site_dirs = sorted([d for d in os.listdir(base_path) \n",
        "                   if os.path.isdir(os.path.join(base_path, d)) and d.startswith('Site')])\n",
        "\n",
        "print(f\"Found {len(site_dirs)} site(s) to analyze:\")\n",
        "for site_dir in site_dirs:\n",
        "    print(f\"  - {site_dir}\")\n",
        "\n",
        "# Store all site metadata\n",
        "all_sites_metadata = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "ANALYZING: Site 01\n",
            "================================================================================\n",
            "\n",
            "  Analyzing: Q01.csv\n",
            "  ----------------------------------------------------------------------------\n",
            "    ✓ Rows: 122,724\n",
            "    ✓ Columns: 6\n",
            "    ✓ Date Range: 2024-04-05 to 2024-06-30\n",
            "    ✓ Years: 2024 - 2024\n",
            "    ✓ Missing Values: 60 (0.05%)\n",
            "    ✓ Duplicate Rows: 0\n",
            "\n",
            "  Analyzing: Q02.csv\n",
            "  ----------------------------------------------------------------------------\n",
            "    ✓ Rows: 131,528\n",
            "    ✓ Columns: 6\n",
            "    ✓ Date Range: 2024-07-01 to 2024-09-30\n",
            "    ✓ Years: 2024 - 2024\n",
            "    ✓ Missing Values: 14 (0.01%)\n",
            "    ✓ Duplicate Rows: 4\n",
            "\n",
            "  Analyzing: Q03.csv\n",
            "  ----------------------------------------------------------------------------\n",
            "    ✓ Rows: 131,852\n",
            "    ✓ Columns: 6\n",
            "    ✓ Date Range: 2024-10-01 to 2024-12-31\n",
            "    ✓ Years: 2024 - 2024\n",
            "    ✓ Missing Values: 52 (0.04%)\n",
            "    ✓ Duplicate Rows: 0\n",
            "\n",
            "  Analyzing: Q04.csv\n",
            "  ----------------------------------------------------------------------------\n",
            "    ✓ Rows: 170,689\n",
            "    ✓ Columns: 6\n",
            "    ✓ Date Range: 2025-01-01 to 2025-04-30\n",
            "    ✓ Years: 2025 - 2025\n",
            "    ✓ Missing Values: 24 (0.01%)\n",
            "    ✓ Duplicate Rows: 0\n",
            "\n",
            "================================================================================\n",
            "SITE SUMMARY: Site 01\n",
            "================================================================================\n",
            "Total Files: 4\n",
            "Total Rows: 556,793\n",
            "Total Missing Values: 150\n",
            "\n",
            "Date Range (All Files):\n",
            "  Start: 2024-04-05\n",
            "  End:   2025-04-30\n",
            "  Total Days: 391\n",
            "\n",
            "Year Coverage: 2024 - 2025\n",
            "  Years: 2024, 2025\n",
            "\n",
            "All Columns Found: day, hour, item_id, minute, w_ghr, w_gtr\n",
            "================================================================================\n",
            "\n",
            "\n",
            "================================================================================\n",
            "ANALYZING: Site 02\n",
            "================================================================================\n",
            "\n",
            "  Analyzing: Q01.csv\n",
            "  ----------------------------------------------------------------------------\n",
            "    ✓ Rows: 130,992\n",
            "    ✓ Columns: 6\n",
            "    ✓ Date Range: 2024-04-01 to 2024-06-30\n",
            "    ✓ Years: 2024 - 2024\n",
            "    ✓ Missing Values: 10 (0.01%)\n",
            "    ✓ Duplicate Rows: 0\n",
            "\n",
            "  Analyzing: Q02.csv\n",
            "  ----------------------------------------------------------------------------\n",
            "    ✓ Rows: 132,305\n",
            "    ✓ Columns: 6\n",
            "    ✓ Date Range: 2024-07-01 to 2024-09-30\n",
            "    ✓ Years: 2024 - 2024\n",
            "    ✓ Missing Values: 21,771 (16.46%)\n",
            "    ✓ Duplicate Rows: 0\n",
            "\n",
            "  Analyzing: Q03.csv\n",
            "  ----------------------------------------------------------------------------\n",
            "    ✓ Rows: 132,111\n",
            "    ✓ Columns: 6\n",
            "    ✓ Date Range: 2024-10-01 to 2024-12-31\n",
            "    ✓ Years: 2024 - 2024\n",
            "    ✓ Missing Values: 1,105 (0.84%)\n",
            "    ✓ Duplicate Rows: 0\n",
            "\n",
            "  Analyzing: Q04.csv\n",
            "  ----------------------------------------------------------------------------\n",
            "    ✓ Rows: 172,376\n",
            "    ✓ Columns: 6\n",
            "    ✓ Date Range: 2025-01-01 to 2025-04-30\n",
            "    ✓ Years: 2025 - 2025\n",
            "    ✓ Missing Values: 64 (0.04%)\n",
            "    ✓ Duplicate Rows: 0\n",
            "\n",
            "================================================================================\n",
            "SITE SUMMARY: Site 02\n",
            "================================================================================\n",
            "Total Files: 4\n",
            "Total Rows: 567,784\n",
            "Total Missing Values: 22,950\n",
            "\n",
            "Date Range (All Files):\n",
            "  Start: 2024-04-01\n",
            "  End:   2025-04-30\n",
            "  Total Days: 395\n",
            "\n",
            "Year Coverage: 2024 - 2025\n",
            "  Years: 2024, 2025\n",
            "\n",
            "All Columns Found: day, hour, item_id, minute, w_ghr, w_gtr\n",
            "================================================================================\n",
            "\n",
            "\n",
            "================================================================================\n",
            "ANALYZING: Site 03\n",
            "================================================================================\n",
            "\n",
            "  Analyzing: Q01.csv\n",
            "  ----------------------------------------------------------------------------\n",
            "    ✓ Rows: 112,801\n",
            "    ✓ Columns: 6\n",
            "    ✓ Date Range: 2024-04-01 to 2024-06-30\n",
            "    ✓ Years: 2024 - 2024\n",
            "    ✓ Missing Values: 0 (0.00%)\n",
            "    ✓ Duplicate Rows: 0\n",
            "\n",
            "  Analyzing: Q02.csv\n",
            "  ----------------------------------------------------------------------------\n",
            "    ✓ Rows: 130,543\n",
            "    ✓ Columns: 6\n",
            "    ✓ Date Range: 2024-07-01 to 2024-09-30\n",
            "    ✓ Years: 2024 - 2024\n",
            "    ✓ Missing Values: 0 (0.00%)\n",
            "    ✓ Duplicate Rows: 0\n",
            "\n",
            "  Analyzing: Q03.csv\n",
            "  ----------------------------------------------------------------------------\n",
            "    ✓ Rows: 129,796\n",
            "    ✓ Columns: 6\n",
            "    ✓ Date Range: 2024-10-01 to 2024-12-31\n",
            "    ✓ Years: 2024 - 2024\n",
            "    ✓ Missing Values: 0 (0.00%)\n",
            "    ✓ Duplicate Rows: 0\n",
            "\n",
            "  Analyzing: Q04.csv\n",
            "  ----------------------------------------------------------------------------\n",
            "    ✓ Rows: 185,797\n",
            "    ✓ Columns: 6\n",
            "    ✓ Date Range: 2025-01-01 to 2025-05-13\n",
            "    ✓ Years: 2025 - 2025\n",
            "    ✓ Missing Values: 0 (0.00%)\n",
            "    ✓ Duplicate Rows: 0\n",
            "\n",
            "================================================================================\n",
            "SITE SUMMARY: Site 03\n",
            "================================================================================\n",
            "Total Files: 4\n",
            "Total Rows: 558,937\n",
            "Total Missing Values: 0\n",
            "\n",
            "Date Range (All Files):\n",
            "  Start: 2024-04-01\n",
            "  End:   2025-05-13\n",
            "  Total Days: 408\n",
            "\n",
            "Year Coverage: 2024 - 2025\n",
            "  Years: 2024, 2025\n",
            "\n",
            "All Columns Found: day, hour, item_id, minute, w_ghr, w_gtr\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Analyze each site\n",
        "for site_dir in site_dirs:\n",
        "    site_path = os.path.join(base_path, site_dir)\n",
        "    site_metadata = analyze_site_data(site_path)\n",
        "    all_sites_metadata[site_dir] = site_metadata\n",
        "    print_site_summary(site_metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Detailed Metadata Access\n",
        "\n",
        "You can access detailed metadata for any site or file using the `all_sites_metadata` dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Detailed Metadata for Site 01 - Q01.csv:\n",
            "{\n",
            "  \"file_name\": \"Q01.csv\",\n",
            "  \"file_path\": \"/Users/IRFAN/Desktop/Irradiance-forecasting/Site Data/Site 01/Q01.csv\",\n",
            "  \"total_rows\": 122724,\n",
            "  \"total_columns\": 6,\n",
            "  \"columns\": [\n",
            "    \"item_id\",\n",
            "    \"day\",\n",
            "    \"hour\",\n",
            "    \"minute\",\n",
            "    \"w_ghr\",\n",
            "    \"w_gtr\"\n",
            "  ],\n",
            "  \"dtypes\": {\n",
            "    \"item_id\": \"object\",\n",
            "    \"day\": \"object\",\n",
            "    \"hour\": \"int64\",\n",
            "    \"minute\": \"int64\",\n",
            "    \"w_ghr\": \"float64\",\n",
            "    \"w_gtr\": \"float64\"\n",
            "  },\n",
            "  \"file_size_mb\": 3.5459346771240234,\n",
            "  \"date_range\": {\n",
            "    \"start_date\": \"2024-04-05\",\n",
            "    \"end_date\": \"2024-06-30\",\n",
            "    \"start_datetime\": \"2024-04-05 00:00:00\",\n",
            "    \"end_datetime\": \"2024-06-30 00:00:00\",\n",
            "    \"total_days\": 87,\n",
            "    \"years\": [\n",
            "      2024\n",
            "    ],\n",
            "    \"year_range\": \"2024 - 2024\",\n",
            "    \"unique_dates\": 87\n",
            "  },\n",
            "  \"missing_values\": {\n",
            "    \"total_missing\": \"60\",\n",
            "    \"columns_with_missing\": {\n",
            "      \"w_ghr\": 30,\n",
            "      \"w_gtr\": 30\n",
            "    },\n",
            "    \"missing_percentage\": {\n",
            "      \"w_ghr\": 0.024445096313679478,\n",
            "      \"w_gtr\": 0.024445096313679478\n",
            "    },\n",
            "    \"complete_rows\": \"122694\",\n",
            "    \"rows_with_missing\": \"30\"\n",
            "  },\n",
            "  \"statistics\": {\n",
            "    \"hour\": {\n",
            "      \"count\": 122724.0,\n",
            "      \"mean\": 11.580310289755875,\n",
            "      \"std\": 6.908836825767589,\n",
            "      \"min\": 0.0,\n",
            "      \"25%\": 6.0,\n",
            "      \"50%\": 12.0,\n",
            "      \"75%\": 18.0,\n",
            "      \"max\": 23.0\n",
            "    },\n",
            "    \"minute\": {\n",
            "      \"count\": 122724.0,\n",
            "      \"mean\": 29.491411622828462,\n",
            "      \"std\": 17.317517625502376,\n",
            "      \"min\": 0.0,\n",
            "      \"25%\": 14.0,\n",
            "      \"50%\": 29.0,\n",
            "      \"75%\": 44.0,\n",
            "      \"max\": 59.0\n",
            "    },\n",
            "    \"w_ghr\": {\n",
            "      \"count\": 122694.0,\n",
            "      \"mean\": 219.09976037948067,\n",
            "      \"std\": 310.1883503133267,\n",
            "      \"min\": 0.0,\n",
            "      \"25%\": 0.0,\n",
            "      \"50%\": 10.0,\n",
            "      \"75%\": 379.0,\n",
            "      \"max\": 1371.0\n",
            "    },\n",
            "    \"w_gtr\": {\n",
            "      \"count\": 122694.0,\n",
            "      \"mean\": 219.6899033367565,\n",
            "      \"std\": 309.4779928286026,\n",
            "      \"min\": 0.0,\n",
            "      \"25%\": 0.0,\n",
            "      \"50%\": 11.0,\n",
            "      \"75%\": 385.0,\n",
            "      \"max\": 1426.0\n",
            "    }\n",
            "  },\n",
            "  \"numeric_columns\": [\n",
            "    \"hour\",\n",
            "    \"minute\",\n",
            "    \"w_ghr\",\n",
            "    \"w_gtr\"\n",
            "  ],\n",
            "  \"quality_checks\": {\n",
            "    \"duplicate_rows\": \"0\",\n",
            "    \"negative_values\": {},\n",
            "    \"zero_values\": {\n",
            "      \"hour\": \"5038\",\n",
            "      \"minute\": \"2044\",\n",
            "      \"w_ghr\": \"59501\",\n",
            "      \"w_gtr\": \"59297\"\n",
            "    },\n",
            "    \"outliers_detected\": {\n",
            "      \"w_ghr\": \"147\",\n",
            "      \"w_gtr\": \"149\"\n",
            "    }\n",
            "  },\n",
            "  \"time_analysis\": {\n",
            "    \"hour_range\": \"0 - 23\",\n",
            "    \"minute_range\": \"0 - 59\",\n",
            "    \"unique_hours\": [\n",
            "      0,\n",
            "      1,\n",
            "      2,\n",
            "      3,\n",
            "      4,\n",
            "      5,\n",
            "      6,\n",
            "      7,\n",
            "      8,\n",
            "      9,\n",
            "      10,\n",
            "      11,\n",
            "      12,\n",
            "      13,\n",
            "      14,\n",
            "      15,\n",
            "      16,\n",
            "      17,\n",
            "      18,\n",
            "      19,\n",
            "      20,\n",
            "      21,\n",
            "      22,\n",
            "      23\n",
            "    ],\n",
            "    \"unique_minutes\": [\n",
            "      0,\n",
            "      1,\n",
            "      2,\n",
            "      3,\n",
            "      4,\n",
            "      5,\n",
            "      6,\n",
            "      7,\n",
            "      8,\n",
            "      9,\n",
            "      10,\n",
            "      11,\n",
            "      12,\n",
            "      13,\n",
            "      14,\n",
            "      15,\n",
            "      16,\n",
            "      17,\n",
            "      18,\n",
            "      19,\n",
            "      20,\n",
            "      21,\n",
            "      22,\n",
            "      23,\n",
            "      24,\n",
            "      25,\n",
            "      26,\n",
            "      27,\n",
            "      28,\n",
            "      29,\n",
            "      30,\n",
            "      31,\n",
            "      32,\n",
            "      33,\n",
            "      34,\n",
            "      35,\n",
            "      36,\n",
            "      37,\n",
            "      38,\n",
            "      39,\n",
            "      40,\n",
            "      41,\n",
            "      42,\n",
            "      43,\n",
            "      44,\n",
            "      45,\n",
            "      46,\n",
            "      47,\n",
            "      48,\n",
            "      49,\n",
            "      50,\n",
            "      51,\n",
            "      52,\n",
            "      53,\n",
            "      54,\n",
            "      55,\n",
            "      56,\n",
            "      57,\n",
            "      58,\n",
            "      59\n",
            "    ],\n",
            "    \"expected_records_per_day\": {\n",
            "      \"count\": 87.0,\n",
            "      \"mean\": 1410.6206896551723,\n",
            "      \"std\": 112.96657680012017,\n",
            "      \"min\": 540.0,\n",
            "      \"25%\": 1439.0,\n",
            "      \"50%\": 1440.0,\n",
            "      \"75%\": 1440.0,\n",
            "      \"max\": 1440.0\n",
            "    }\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Example: View detailed metadata for Site 01, Q01.csv\n",
        "if 'Site 01' in all_sites_metadata and 'Q01.csv' in all_sites_metadata['Site 01']['files']:\n",
        "    import json\n",
        "    q01_meta = all_sites_metadata['Site 01']['files']['Q01.csv']\n",
        "    print(\"Detailed Metadata for Site 01 - Q01.csv:\")\n",
        "    print(json.dumps(q01_meta, indent=2, default=str))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Data Overview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Summary Across All Sites:\n",
            "================================================================================\n",
            "   Site  Total Files  Total Rows  Total Missing  Year Range               Date Range\n",
            "Site 01            4      556793            150 2024 - 2025 2024-04-05 to 2025-04-30\n",
            "Site 02            4      567784          22950 2024 - 2025 2024-04-01 to 2025-04-30\n",
            "Site 03            4      558937              0 2024 - 2025 2024-04-01 to 2025-05-13\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Create a summary DataFrame for all sites\n",
        "summary_data = []\n",
        "\n",
        "for site_name, site_meta in all_sites_metadata.items():\n",
        "    summary = site_meta['summary']\n",
        "    summary_data.append({\n",
        "        'Site': site_name,\n",
        "        'Total Files': summary['total_files'],\n",
        "        'Total Rows': summary['total_rows'],\n",
        "        'Total Missing': summary['total_missing_values'],\n",
        "        'Year Range': summary.get('year_range', 'N/A'),\n",
        "        'Date Range': f\"{summary['date_range_all'].get('start_str', 'N/A')} to {summary['date_range_all'].get('end_str', 'N/A')}\"\n",
        "    })\n",
        "\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "print(\"\\nSummary Across All Sites:\")\n",
        "print(\"=\"*80)\n",
        "print(summary_df.to_string(index=False))\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Detailed File-Level Summary:\n",
            "====================================================================================================\n",
            "   Site    File   Rows  Missing Values  Date Range Years  Duplicate Rows\n",
            "Site 01 Q01.csv 122724              60 2024 - 2024  2024               0\n",
            "Site 01 Q02.csv 131528              14 2024 - 2024  2024               4\n",
            "Site 01 Q03.csv 131852              52 2024 - 2024  2024               0\n",
            "Site 01 Q04.csv 170689              24 2025 - 2025  2025               0\n",
            "Site 02 Q01.csv 130992              10 2024 - 2024  2024               0\n",
            "Site 02 Q02.csv 132305           21771 2024 - 2024  2024               0\n",
            "Site 02 Q03.csv 132111            1105 2024 - 2024  2024               0\n",
            "Site 02 Q04.csv 172376              64 2025 - 2025  2025               0\n",
            "Site 03 Q01.csv 112801               0 2024 - 2024  2024               0\n",
            "Site 03 Q02.csv 130543               0 2024 - 2024  2024               0\n",
            "Site 03 Q03.csv 129796               0 2024 - 2024  2024               0\n",
            "Site 03 Q04.csv 185797               0 2025 - 2025  2025               0\n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Create detailed file-level summary\n",
        "file_summary_data = []\n",
        "\n",
        "for site_name, site_meta in all_sites_metadata.items():\n",
        "    for file_name, file_meta in site_meta['files'].items():\n",
        "        if 'error' not in file_meta:\n",
        "            file_summary_data.append({\n",
        "                'Site': site_name,\n",
        "                'File': file_name,\n",
        "                'Rows': file_meta.get('total_rows', 0),\n",
        "                'Missing Values': file_meta.get('missing_values', {}).get('total_missing', 0),\n",
        "                'Date Range': file_meta.get('date_range', {}).get('year_range', 'N/A'),\n",
        "                'Years': ', '.join(map(str, file_meta.get('date_range', {}).get('years', []))),\n",
        "                'Duplicate Rows': file_meta.get('quality_checks', {}).get('duplicate_rows', 0)\n",
        "            })\n",
        "\n",
        "file_summary_df = pd.DataFrame(file_summary_data)\n",
        "print(\"\\nDetailed File-Level Summary:\")\n",
        "print(\"=\"*100)\n",
        "print(file_summary_df.to_string(index=False))\n",
        "print(\"=\"*100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Missing Values Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Missing Values Analysis by Site and File:\n",
            "====================================================================================================\n",
            "\n",
            "Site 01:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "  Q01.csv:\n",
            "    Total Missing: 60\n",
            "      - w_ghr: 30 (0.02%)\n",
            "      - w_gtr: 30 (0.02%)\n",
            "  Q02.csv:\n",
            "    Total Missing: 14\n",
            "      - w_ghr: 7 (0.01%)\n",
            "      - w_gtr: 7 (0.01%)\n",
            "  Q03.csv:\n",
            "    Total Missing: 52\n",
            "      - w_ghr: 26 (0.02%)\n",
            "      - w_gtr: 26 (0.02%)\n",
            "  Q04.csv:\n",
            "    Total Missing: 24\n",
            "      - w_ghr: 12 (0.01%)\n",
            "      - w_gtr: 12 (0.01%)\n",
            "\n",
            "Site 02:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "  Q01.csv:\n",
            "    Total Missing: 10\n",
            "      - w_ghr: 5 (0.00%)\n",
            "      - w_gtr: 5 (0.00%)\n",
            "  Q02.csv:\n",
            "    Total Missing: 21,771\n",
            "      - w_ghr: 21,760 (16.45%)\n",
            "      - w_gtr: 11 (0.01%)\n",
            "  Q03.csv:\n",
            "    Total Missing: 1,105\n",
            "      - w_ghr: 1,065 (0.81%)\n",
            "      - w_gtr: 40 (0.03%)\n",
            "  Q04.csv:\n",
            "    Total Missing: 64\n",
            "      - w_ghr: 32 (0.02%)\n",
            "      - w_gtr: 32 (0.02%)\n",
            "\n",
            "Site 03:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "  Q01.csv: No missing values ✓\n",
            "  Q02.csv: No missing values ✓\n",
            "  Q03.csv: No missing values ✓\n",
            "  Q04.csv: No missing values ✓\n"
          ]
        }
      ],
      "source": [
        "# Detailed missing values analysis\n",
        "print(\"\\nMissing Values Analysis by Site and File:\")\n",
        "print(\"=\"*100)\n",
        "\n",
        "for site_name, site_meta in all_sites_metadata.items():\n",
        "    print(f\"\\n{site_name}:\")\n",
        "    print(\"-\" * 100)\n",
        "    for file_name, file_meta in site_meta['files'].items():\n",
        "        if 'error' not in file_meta:\n",
        "            missing_info = file_meta.get('missing_values', {})\n",
        "            if missing_info.get('total_missing', 0) > 0:\n",
        "                print(f\"  {file_name}:\")\n",
        "                print(f\"    Total Missing: {missing_info['total_missing']:,}\")\n",
        "                if missing_info.get('columns_with_missing'):\n",
        "                    for col, count in missing_info['columns_with_missing'].items():\n",
        "                        pct = missing_info['missing_percentage'].get(col, 0)\n",
        "                        print(f\"      - {col}: {count:,} ({pct:.2f}%)\")\n",
        "            else:\n",
        "                print(f\"  {file_name}: No missing values ✓\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load and Preview Sample Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "Sample Data from Site 01\n",
            "================================================================================\n",
            "\n",
            "First 10 rows from Q01.csv:\n",
            "item_id        day  hour  minute  w_ghr  w_gtr\n",
            "    WMS 2024-04-05    15       0  813.0  873.0\n",
            "    WMS 2024-04-05    15       1  795.0  857.0\n",
            "    WMS 2024-04-05    15       2  799.0  860.0\n",
            "    WMS 2024-04-05    15       3  808.0  869.0\n",
            "    WMS 2024-04-05    15       4  814.0  873.0\n",
            "    WMS 2024-04-05    15       5  800.0  862.0\n",
            "    WMS 2024-04-05    15       6  794.0  860.0\n",
            "    WMS 2024-04-05    15       7  793.0  856.0\n",
            "    WMS 2024-04-05    15       8  778.0  843.0\n",
            "    WMS 2024-04-05    15       9  773.0  842.0\n",
            "\n",
            "Data types:\n",
            "item_id     object\n",
            "day         object\n",
            "hour         int64\n",
            "minute       int64\n",
            "w_ghr      float64\n",
            "w_gtr      float64\n",
            "dtype: object\n",
            "\n",
            "Shape: 10 rows × 6 columns\n",
            "\n",
            "================================================================================\n",
            "Sample Data from Site 02\n",
            "================================================================================\n",
            "\n",
            "First 10 rows from Q01.csv:\n",
            "item_id        day  hour  minute  w_ghr  w_gtr\n",
            "    WMS 2024-04-01     0       0   -3.0   -4.0\n",
            "    WMS 2024-04-01     0       1   -3.0   -4.0\n",
            "    WMS 2024-04-01     0       2   -3.0   -4.0\n",
            "    WMS 2024-04-01     0       3   -3.0   -4.0\n",
            "    WMS 2024-04-01     0       4   -3.0   -3.0\n",
            "    WMS 2024-04-01     0       5   -3.0   -3.0\n",
            "    WMS 2024-04-01     0       6   -3.0   -3.0\n",
            "    WMS 2024-04-01     0       7   -3.0   -3.0\n",
            "    WMS 2024-04-01     0       8   -3.0   -3.0\n",
            "    WMS 2024-04-01     0       9   -3.0   -3.0\n",
            "\n",
            "Data types:\n",
            "item_id     object\n",
            "day         object\n",
            "hour         int64\n",
            "minute       int64\n",
            "w_ghr      float64\n",
            "w_gtr      float64\n",
            "dtype: object\n",
            "\n",
            "Shape: 10 rows × 6 columns\n",
            "\n",
            "================================================================================\n",
            "Sample Data from Site 03\n",
            "================================================================================\n",
            "\n",
            "First 10 rows from Q01.csv:\n",
            "item_id        day  hour  minute  w_ghr  w_gtr\n",
            "MCR_WMS 2024-04-01     0       0   -3.0   -4.0\n",
            "MCR_WMS 2024-04-01     0       1   -3.0   -4.0\n",
            "MCR_WMS 2024-04-01     0       2   -3.0   -4.0\n",
            "MCR_WMS 2024-04-01     0       3   -3.0   -4.0\n",
            "MCR_WMS 2024-04-01     0       4   -3.0   -4.0\n",
            "MCR_WMS 2024-04-01     0       5   -3.0   -4.0\n",
            "MCR_WMS 2024-04-01     0       6   -3.0   -4.0\n",
            "MCR_WMS 2024-04-01     0       7   -3.0   -3.0\n",
            "MCR_WMS 2024-04-01     0       8   -3.0   -3.0\n",
            "MCR_WMS 2024-04-01     0       9   -3.0   -3.0\n",
            "\n",
            "Data types:\n",
            "item_id     object\n",
            "day         object\n",
            "hour         int64\n",
            "minute       int64\n",
            "w_ghr      float64\n",
            "w_gtr      float64\n",
            "dtype: object\n",
            "\n",
            "Shape: 10 rows × 6 columns\n"
          ]
        }
      ],
      "source": [
        "# Load and display sample data from each site\n",
        "for site_name, site_meta in all_sites_metadata.items():\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Sample Data from {site_name}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    # Get the first file from each site\n",
        "    first_file = sorted(site_meta['files'].keys())[0] if site_meta['files'] else None\n",
        "    if first_file and 'error' not in site_meta['files'][first_file]:\n",
        "        file_path = site_meta['files'][first_file]['file_path']\n",
        "        df_sample = pd.read_csv(file_path, nrows=10)\n",
        "        print(f\"\\nFirst 10 rows from {first_file}:\")\n",
        "        print(df_sample.to_string(index=False))\n",
        "        print(f\"\\nData types:\")\n",
        "        print(df_sample.dtypes)\n",
        "        print(f\"\\nShape: {df_sample.shape[0]} rows × {df_sample.shape[1]} columns\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Time Interval Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "TIME INTERVAL ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "Sample Data Structure:\n",
            "  Columns: item_id, day, hour, minute, w_ghr, w_gtr, datetime, time_diff\n",
            "\n",
            "Time Interval Analysis:\n",
            "  Most common time difference: 0 days 00:01:00\n",
            "  Time interval: 1 minute (60 seconds)\n",
            "\n",
            "Minute values range: 0 to 59\n",
            "Hour values range: 15 to 18\n",
            "\n",
            "Expected records per hour: 60 (one per minute)\n",
            "Expected records per day: 1,440 (24 hours × 60 minutes)\n",
            "\n",
            "Data Continuity Check:\n",
            "  First timestamp: 2024-04-05 15:00:00\n",
            "  Last timestamp (in sample): 2024-04-05 18:19:00\n",
            "  Total records in sample: 200\n",
            "  Expected records (if continuous): 200\n",
            "\n",
            "Time Difference Distribution (first 10):\n",
            "  0 days 00:01:00: 199 occurrences\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Analyze time interval in the data\n",
        "print(\"=\"*80)\n",
        "print(\"TIME INTERVAL ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Check a sample file to determine time interval\n",
        "sample_file = \"/Users/IRFAN/Desktop/Irradiance-forecasting/Site Data/Site 01/Q01.csv\"\n",
        "df_sample = pd.read_csv(sample_file, nrows=200)\n",
        "\n",
        "# Create datetime column\n",
        "df_sample['day'] = pd.to_datetime(df_sample['day'])\n",
        "df_sample['datetime'] = pd.to_datetime(\n",
        "    df_sample['day'].astype(str) + ' ' + \n",
        "    df_sample['hour'].astype(str) + ':' + \n",
        "    df_sample['minute'].astype(str).str.zfill(2)\n",
        ")\n",
        "\n",
        "# Calculate time differences\n",
        "df_sample['time_diff'] = df_sample['datetime'].diff()\n",
        "\n",
        "# Get the most common time difference\n",
        "most_common_diff = df_sample['time_diff'].mode()[0] if len(df_sample['time_diff'].mode()) > 0 else None\n",
        "\n",
        "print(f\"\\nSample Data Structure:\")\n",
        "print(f\"  Columns: {', '.join(df_sample.columns)}\")\n",
        "print(f\"\\nTime Interval Analysis:\")\n",
        "print(f\"  Most common time difference: {most_common_diff}\")\n",
        "print(f\"  Time interval: 1 minute (60 seconds)\")\n",
        "print(f\"\\nMinute values range: {df_sample['minute'].min()} to {df_sample['minute'].max()}\")\n",
        "print(f\"Hour values range: {df_sample['hour'].min()} to {df_sample['hour'].max()}\")\n",
        "print(f\"\\nExpected records per hour: 60 (one per minute)\")\n",
        "print(f\"Expected records per day: 1,440 (24 hours × 60 minutes)\")\n",
        "\n",
        "# Check if data is continuous\n",
        "print(f\"\\nData Continuity Check:\")\n",
        "print(f\"  First timestamp: {df_sample['datetime'].iloc[0]}\")\n",
        "print(f\"  Last timestamp (in sample): {df_sample['datetime'].iloc[-1]}\")\n",
        "print(f\"  Total records in sample: {len(df_sample)}\")\n",
        "print(f\"  Expected records (if continuous): {(df_sample['datetime'].iloc[-1] - df_sample['datetime'].iloc[0]).total_seconds() / 60 + 1:.0f}\")\n",
        "\n",
        "# Count unique time differences\n",
        "time_diff_counts = df_sample['time_diff'].value_counts().head(10)\n",
        "print(f\"\\nTime Difference Distribution (first 10):\")\n",
        "for diff, count in time_diff_counts.items():\n",
        "    if pd.notna(diff):\n",
        "        print(f\"  {diff}: {count} occurrences\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
